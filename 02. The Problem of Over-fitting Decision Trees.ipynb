{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"02. The Problem of Over-fitting Decision Trees.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5soGuirloNgq"},"source":["# Computational Astrophysics 2021\n","---\n","## Eduard Larrañaga\n","\n","Observatorio Astronómico Nacional\\\n","Facultad de Ciencias\\\n","Universidad Nacional de Colombia\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Ff4KTapaoNgx"},"source":["## 02. The Problem of Over-Fitting the Decision Trees\n","### About this notebook\n","\n","In this worksheet we will illustrate the problem of over-fitting decision trees.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Y85Y_aIb8meo"},"source":["Over-fitting the data means that an algorithm tries to incorporate (all) the outlying data points, which implies that the prediction accuracy of the general trend is diminished. \n","\n","\n","In this worksheet, we will illustrate how decision trees tend to overfit the data if they are left unsupervised. We will use the same dataset of galaxies.\n"]},{"cell_type":"markdown","metadata":{"id":"cn0RkhCyHfEO"},"source":["### Loading the Data\n","\n","As before, we will use the dataset provided as a NumPy strctured array in a binary format (.npy) called 'sdss_galaxy_colors.npy'. \n"]},{"cell_type":"code","metadata":{"id":"z2_uSWSg5VRk"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ob2DGMZ1o7Dp"},"source":["path='' #Define an empty string to use in case of local working"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9lFN5Fho-ei","executionInfo":{"status":"ok","timestamp":1612141645150,"user_tz":300,"elapsed":20323,"user":{"displayName":"Eduard Alexis Larranaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCVABzEgj-rCdyxWa29RnA0kIYUCXAaVbnRYOEhQ=s64","userId":"04402438389940282602"}},"outputId":"88c78ae0-f3c4-4f03-8438-f8171ec87223"},"source":["# Working with google colab needs to mount the Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"18nqDWVKpGb7"},"source":["# we define the path to the files\n","path = '/content/drive/MyDrive/Colab Notebooks/CA2021/11. Decision Trees/presentation/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t00prl26IXxz","executionInfo":{"status":"ok","timestamp":1612141751866,"user_tz":300,"elapsed":446,"user":{"displayName":"Eduard Alexis Larranaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCVABzEgj-rCdyxWa29RnA0kIYUCXAaVbnRYOEhQ=s64","userId":"04402438389940282602"}},"outputId":"e0aa12a1-69a1-4b24-b329-457e4bdc358e"},"source":["data = np.load(path+'sdss_galaxy_colors.npy')\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([(19.84132, 19.52656, 19.46946, 19.17955, 19.10763, b'QSO', 0.539301  , 6.543622e-05),\n","       (19.86318, 18.66298, 17.84272, 17.38978, 17.14313, b'GALAXY', 0.1645703 , 1.186625e-05),\n","       (19.97362, 18.31421, 17.47922, 17.0744 , 16.76174, b'GALAXY', 0.04190006, 2.183788e-05),\n","       ...,\n","       (19.82667, 18.10038, 17.16133, 16.5796 , 16.19755, b'GALAXY', 0.0784592 , 2.159406e-05),\n","       (19.98672, 19.75385, 19.5713 , 19.27739, 19.25895, b'QSO', 1.567295  , 4.505933e-04),\n","       (18.00024, 17.80957, 17.77302, 17.72663, 17.7264 , b'QSO', 0.4749449 , 6.203324e-05)],\n","      dtype=[('u', '<f8'), ('g', '<f8'), ('r', '<f8'), ('i', '<f8'), ('z', '<f8'), ('spec_class', 'S6'), ('redshift', '<f8'), ('redshift_err', '<f8')])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"C7pkVeizIyj5"},"source":["In this kind of data structure, the `dtype` attribute corresponds to the name of the features. For our example, we identify the following:\n","\n","| dtype | Feature|\n","|:-:|:-:|\n","|`u` |u band filter|\n","|`g` |g band filter|\n","|`r` |r band filter|\n","|`i` |i band filter|\n","|`z` |z band filter|\n","|`spec_class` |spectral class|\n","|`redshift` |redshift|\n","|`redshift_err` |redshift error|\n"]},{"cell_type":"markdown","metadata":{"id":"ca9ZEeaNK8Vh"},"source":["The number of samples (galaxies) in this dataset is"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JHyXtPlK6Hs","executionInfo":{"status":"ok","timestamp":1612142762690,"user_tz":300,"elapsed":536,"user":{"displayName":"Eduard Alexis Larranaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCVABzEgj-rCdyxWa29RnA0kIYUCXAaVbnRYOEhQ=s64","userId":"04402438389940282602"}},"outputId":"12892e14-8457-4cd4-d2a9-4befaa94a8c6"},"source":["n = data.size\n","n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"geu4TFRioNgx"},"source":["---\n","### Training the Decision Tree\n","\n","As we have seen, decision trees have many advantages such as \n","- They are simple to implement\n","- They are easy to interpret\n","- The data does not require too much preparation\n","- Decision trees are (usually) computationally efficient.\n","\n","However, decision trees also have some limitations.One of the biggest is that they tend to over-fit the data if they are not checked. The over-fitting means that they will create a super-complicated tree that attempts to account for (all) the outliers in the data. This problem appears because the algorithm tries to optimise the decision locally at each node. \n"]},{"cell_type":"markdown","metadata":{"id":"675UL8zdLlbc"},"source":["In order to implement the decision tree, we will use the functions defined in the previous worksheet to define the features (4 color indices) and the targets (redshift)"]},{"cell_type":"code","metadata":{"id":"Q53oIok9eCgL"},"source":["# Function returning the 4 color indices and the redshifts\n","\n","features, targets = ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DjjbFANBHcD"},"source":["We will also split the data into training and testing subsets. You can choose the size of the split (if in doubt, a 50:50 split is fine)."]},{"cell_type":"code","metadata":{"id":"JTULQCiYZ8U-"},"source":["split_n = n//2\n","train_features = features[:split]\n","test_features = features[split:]\n","train_targets = ...\n","trest_targets = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNehsBLLFWUH"},"source":["As before, we will use the function `sklearn.tree.DecisionTreeRegressor`.\n","\n","However, in order to reduce the over-fitting, we can constrain the number of decision node rows, called the **tree depth**. We can control the depth of decision tree learned, using an argument in the `DecisionTreeRegressor`function. \n","\n","To set the maximum depth to 5 we use"]},{"cell_type":"code","metadata":{"id":"mG9255X6fcdH"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","dec_tree = DecisionTreeRegressor(max_depth=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twCZM4TefbG5"},"source":["\n","The decision tree is trained using the method `.fit()` and the defined train subsets of the 'features' and 'targets' arrays. \n","\n","Detailed information about this function is available at\n","\n","https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"]},{"cell_type":"markdown","metadata":{"id":"jXuqBDnbfS31"},"source":["The decision tree is trained using the method `.fit()` and the training subsets of the arrays 'features' and 'targets',\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DcJwQ_8bgmdW"},"source":["dec_tree.fit(train_features, train_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4-CuGQ2YDt6"},"source":["#### Testing the Decision Tree\n","\n","Once the decision tree is ready, we will apply the method `.predict()` to the test subset."]},{"cell_type":"code","metadata":{"id":"hV_1GWdc6GL6"},"source":["predictions = dec_tree.predict(test_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OniL8coCYm9-"},"source":["In order to evaluate the decision tree, we will use again the median of the differences between the predictions and the target values, i.e.\n","\n","\\begin{equation}\n","\\text{eval_dec_tree} = \\text{median}\\left\\lbrace \\left| \\text{predictions}_i - \\text{targets}_i \\right|\\right\\rbrace\n","\\end{equation}\n","\n","Use the same function defined in the previous worksheet. "]},{"cell_type":"code","metadata":{"id":"SDLt8C-IZJUG"},"source":["eval_dec_tree = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGdRXf17ZwKD"},"source":["#### Over-Fitting and Tree Depth\n","\n","In order to see how the tree is overfitting the data, we will examine how the decision tree performs for different tree depths. \n","\n","It may be expected that, the deeper the tree, the better it should perform. However, we will show that as the model overfits, there is an important difference in the accuracy of the prediction when applied to the training data and to the testing data.\n","\n","**1. Define a function that creates a decision tree with depths in the range 0 to 40. The function must use the decision tree to predict the redshift of the training and test subsets and calculate the corresponding median of the differences to evaluate the algorithm.**\n","\n","**2. Plot the median of the differences vs tree depth.**\n","\n","The plot should look like this\n","\n","<center>\n","<img src=\"https://groklearning-cdn.com/problems/8Cet6iLGMbP2L8t7SVkEEg/overfitting.png\" width=450>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"nUY7ZcLrAQOT"},"source":["The above graphic shows thhoew the accuracy of the decision tree on the **training set** gets better as we allow the tree to grow to greater depths. **In fact, at a depth of around 27, the errors goes to zero!**\n","\n","On the other hand, the accuracy measure of the predictions for the **test set** gets better initially but then it gets worse at larger tree depths. Hence, this plot shows how at a tree depth of around 19, the decision tree starts to **overfit** the data!. This happens because the algorithm tries to include outliers in the training set and it produces a decrease in its general predictive accuracy.\n","\n","In order to prevent the over-fitting problem, we note that the the better value of the accuracy of the predictions for the test set is around 19 or 20 and therefore, we will adjust the tree depth to this value. "]}]}